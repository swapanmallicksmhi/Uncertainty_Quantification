#!/usr/bin/env python3
"""
grib_to_daily_nc.py

Group GRIB files by day and write one NetCDF per day containing variables:
  - t2m
  - sp
  - u10
  - v10

Usage:
    python grib_to_daily_nc.py input_dir out_dir
"""
import sys
import os
import re
import numpy as np
import xarray as xr
from collections import defaultdict
from datetime import datetime

# Try to import pygrib (preferred)
try:
    import pygrib
    HAVE_PYGRIB = True
except Exception:
    HAVE_PYGRIB = False

def safe_makedirs(path):
    os.makedirs(path, exist_ok=True)

def to_np_datetime_list(dt_list):
    out = []
    for d in dt_list:
        if d is None:
            out.append(np.datetime64("NaT"))
        elif isinstance(d, np.datetime64):
            out.append(d)
        elif isinstance(d, datetime):
            out.append(np.datetime64(d))
        else:
            try:
                out.append(np.datetime64(d))
            except Exception:
                out.append(np.datetime64("NaT"))
    return np.array(out, dtype="datetime64[ns]")

# map common GRIB shortNames to desired variable names
SHORTNAME_MAP = {
    "2t": "t2m", "t2m": "t2m", "t": "t2m",
    "msl": "sp", "sp": "sp",
    "10u": "u10", "u10": "u10",
    "10v": "v10", "v10": "v10"
}

def infer_date_from_filename(fname):
    m = re.search(r"(\d{8})", fname)
    if m:
        return m.group(1)  # YYYYMMDD
    return None

def process_file_pygrib(path, perday_store):
    """
    Read a single GRIB file with pygrib and append arrays into perday_store.
    perday_store is: perday_store[date_key][varname] -> list of tuples (valid_dt, array, lat2d, lon2d)
    """
    try:
        grbs = pygrib.open(path)
    except Exception as e:
        print(f"  ERROR opening {path} with pygrib: {e}")
        return

    # try extracting date from filename
    fbasename = os.path.basename(path)
    file_date = infer_date_from_filename(fbasename)

    idx = 0
    for msg in grbs:
        short = getattr(msg, "shortName", None)
        if short is None:
            idx += 1
            continue
        short_lower = short.lower()
        varname = SHORTNAME_MAP.get(short_lower)
        if not varname:
            # skip variables we don't care about
            idx += 1
            continue

        # read values, lat/lon, valid datetime
        try:
            vals = np.array(msg.values, dtype=np.float32)
        except Exception as e:
            print(f"    Warning: cannot read values for msg idx {idx}: {e}")
            idx += 1
            continue

        try:
            lats, lons = msg.latlons()
            lat2d = np.asarray(lats, dtype=np.float32)
            lon2d = np.asarray(lons, dtype=np.float32)
        except Exception:
            lat2d = None
            lon2d = None

        valid_dt = getattr(msg, "validDate", None)
        if valid_dt is None:
            # try dataDate + dataTime
            dd = getattr(msg, "dataDate", None)
            dt = getattr(msg, "dataTime", None)
            if dd and dt is not None:
                try:
                    y = dd // 10000
                    m = (dd // 100) % 100
                    d = dd % 100
                    hr = dt // 100
                    valid_dt = datetime(y, m, d, hr)
                except Exception:
                    valid_dt = None

        # final fallback: parse from filename if no valid_dt
        if valid_dt is None and file_date is not None:
            # if filename only contains date, try to get hour from message if present
            # otherwise set to date at 00:00
            try:
                valid_dt = datetime.strptime(file_date, "%Y%m%d")
            except Exception:
                valid_dt = None

        # determine day key (YYYYMMDD) for grouping
        if valid_dt is not None:
            daykey = valid_dt.strftime("%Y%m%d")
        elif file_date is not None:
            daykey = file_date
        else:
            # fallback to file modification date
            daykey = datetime.utcfromtimestamp(os.path.getmtime(path)).strftime("%Y%m%d")

        perday_store[daykey][varname].append((valid_dt, vals, lat2d, lon2d))
        idx += 1

    grbs.close()


def process_file_cfgrib(path, perday_store):
    """
    Best-effort fallback using cfgrib / xarray for files where pygrib is not available.
    We open the file and try to extract variables named like the SHORTNAME_MAP keys.
    """
    try:
        ds_all = xr.open_dataset(path, engine="cfgrib")
    except Exception as e:
        print(f"  ERROR opening {path} with cfgrib: {e}")
        return

    fbasename = os.path.basename(path)
    file_date = infer_date_from_filename(fbasename)

    # iterate variables in ds_all and extract those that map
    for var in ds_all.data_vars:
        var_low = var.lower()
        mapped = None
        for k, v in SHORTNAME_MAP.items():
            # cfgrib names may differ; check if target variable name in var or viceversa
            if var_low == k or var_low == v:
                mapped = v
                break
        if mapped is None:
            # try to match common names
            if var_low in ("t", "t2m", "2t"):
                mapped = "t2m"
            elif var_low in ("msl", "sp"):
                mapped = "sp"
            elif var_low in ("u10", "10u"):
                mapped = "u10"
            elif var_low in ("v10", "10v"):
                mapped = "v10"
        if mapped is None:
            continue

        arr = ds_all[var].values
        # ensure 2D: if dims include time, take first time
        if arr.ndim == 3:
            # (time, y, x) -> take first time (file probably single time)
            arr2 = np.asarray(arr[0, ...], dtype=np.float32)
            valid_dt = None
        else:
            arr2 = np.asarray(arr, dtype=np.float32)
            valid_dt = None

        # attempt to extract valid datetime from coords/attrs
        try:
            if "time" in ds_all.coords:
                tvals = ds_all.coords["time"].values
                if np.ndim(tvals) and len(tvals) > 0:
                    # take first value
                    try:
                        valid_dt = np.datetime64(tvals[0]).astype("datetime64[s]").tolist()
                    except Exception:
                        valid_dt = None
        except Exception:
            valid_dt = None

        # lat/lon
        lat2d = None; lon2d = None
        if "latitude" in ds_all.coords and "longitude" in ds_all.coords:
            try:
                lat2d = ds_all["latitude"].values
                lon2d = ds_all["longitude"].values
            except Exception:
                lat2d = lon2d = None

        if valid_dt is not None:
            try:
                # convert numpy datetime64 to python datetime
                if isinstance(valid_dt, np.datetime64):
                    valid_dt = valid_dt.astype("datetime64[s]").tolist()
            except Exception:
                pass

        if valid_dt is None and file_date is not None:
            try:
                valid_dt = datetime.strptime(file_date, "%Y%m%d")
            except Exception:
                valid_dt = None

        if valid_dt is not None:
            daykey = valid_dt.strftime("%Y%m%d")
        elif file_date is not None:
            daykey = file_date
        else:
            daykey = datetime.utcfromtimestamp(os.path.getmtime(path)).strftime("%Y%m%d")

        perday_store[daykey][mapped].append((valid_dt, arr2, lat2d, lon2d))

    ds_all.close()


def assemble_and_write_day(daykey, entries, out_dir):
    """
    entries: dict varname -> list of (valid_dt, array, lat2d, lon2d)
    Writes one netcdf per day with variables t2m, sp, u10, v10 stacked along time.
    """
    wanted_vars = ["t2m", "sp", "u10", "v10"]
    present_vars = [v for v in wanted_vars if v in entries and len(entries[v]) > 0]

    if not present_vars:
        print(f"  No wanted variables found for day {daykey}, skipping.")
        return

    # For each variable, sort by valid_dt and stack arrays into shape (T, ny, nx)
    var_arrays = {}
    var_valids = {}
    lat2d = lon2d = None
    for v in present_vars:
        recs = entries[v]
        # filter out records without arrays
        recs = [r for r in recs if r[1] is not None]
        if len(recs) == 0:
            continue
        # sort by valid_dt (None goes to end)
        recs_sorted = sorted(recs, key=lambda it: (it[0] is None, it[0]))
        # ensure consistent shape
        shapes = [r[1].shape for r in recs_sorted]
        # pick reference shape
        ref_shape = shapes[0]
        filtered = []
        for r in recs_sorted:
            if r[1].shape == ref_shape:
                filtered.append(r)
            else:
                print(f"    Warning: skipping one {v} record due to shape mismatch {r[1].shape} != {ref_shape}")
        if len(filtered) == 0:
            continue

        arrays = np.stack([r[1].astype(np.float32) for r in filtered], axis=0)
        valids = [r[0] for r in filtered]
        var_arrays[v] = arrays
        var_valids[v] = valids

        # capture lat/lon if available
        for r in filtered:
            if r[2] is not None and r[3] is not None:
                lat2d = r[2] if lat2d is None else lat2d
                lon2d = r[3] if lon2d is None else lon2d

    # Determine common time axis: prefer using t2m valids if present else union of all valids sorted
    all_valids = []
    for v in var_valids:
        all_valids.extend([d for d in var_valids[v] if d is not None])
    # unique and sorted
    unique_valids = sorted(list({d for d in all_valids if d is not None}))
    if len(unique_valids) == 0:
        # fallback to integer time axis of max length among variables
        maxT = max(arr.shape[0] for arr in var_arrays.values())
        time_seq = np.arange(1, maxT + 1, dtype=np.int32)
        valid_datetime_coord = None
    else:
        time_seq = np.arange(1, len(unique_valids) + 1, dtype=np.int32)
        valid_datetime_coord = to_np_datetime_list(unique_valids)

    # Create dataset coords
    # Take y,x from any var_arrays
    sample_var = next(iter(var_arrays.values()))
    T_sample, ny, nx = sample_var.shape

    coords = {
        "time": ("time", time_seq),
        "y": ("y", np.arange(ny, dtype=np.int32)),
        "x": ("x", np.arange(nx, dtype=np.int32))
    }

    data_vars = {}
    # For each wanted var create an array aligned to the common time axis.
    for v in wanted_vars:
        if v not in var_arrays:
            continue
        arr = var_arrays[v]  # shape (Tv, ny, nx)
        # if we have valid datetimes for this var, map them to the index in unique_valids
        if v in var_valids and len(var_valids[v]) > 0 and len(unique_valids) > 0:
            idx_map = []
            for d in var_valids[v]:
                if d is None:
                    idx_map.append(None)
                else:
                    try:
                        idx_map.append(unique_valids.index(d))
                    except ValueError:
                        idx_map.append(None)
            # build an array of shape (len(unique_valids), ny, nx) filled with nan and populate
            stacked = np.full((len(unique_valids), ny, nx), np.nan, dtype=np.float32)
            arr_t = arr
            for i_src, dstpos in enumerate(idx_map):
                if dstpos is None:
                    # append to the end sequentially if None; find first free slot (naive)
                    # Find first index in stacked which is all-nan
                    empties = np.where(np.all(np.isnan(stacked), axis=(1,2)))[0]
                    if empties.size > 0:
                        stacked[empties[0], :, :] = arr_t[i_src]
                else:
                    stacked[dstpos, :, :] = arr_t[i_src]
            data_vars[v] = (("time", "y", "x"), stacked)
        else:
            # no valid datetimes: if arr.shape[0] matches time_seq len use it, else try to pad/truncate
            if arr.shape[0] == time_seq.size:
                data_vars[v] = (("time", "y", "x"), arr)
            elif arr.shape[0] < time_seq.size:
                stacked = np.full((time_seq.size, ny, nx), np.nan, dtype=np.float32)
                stacked[:arr.shape[0], :, :] = arr
                data_vars[v] = (("time", "y", "x"), stacked)
            else:
                # more time steps than common axis â€” truncate
                data_vars[v] = (("time", "y", "x"), arr[:time_seq.size, :, :])

    ds = xr.Dataset(data_vars=data_vars, coords=coords)

    # assign valid_datetime if present
    if valid_datetime_coord is not None:
        ds = ds.assign_coords(valid_datetime=("time", valid_datetime_coord))
    else:
        ds = ds.assign_coords(valid_time=("time", time_seq))

    # add lat/lon if present
    if lat2d is not None and lon2d is not None:
        # ensure shapes match y,x
        ds["lat"] = (("y", "x"), lat2d)
        ds["lon"] = (("y", "x"), lon2d)

    # add attrs
    ds.attrs["source"] = "converted by grib_to_daily_nc.py"
    out_name = f"CARRA2_SFC_{daykey}.nc"
    out_path = os.path.join(out_dir, out_name)
    print(f"  Writing {out_path}  vars={list(ds.data_vars.keys())} shape(time,y,x) sample={list(ds.data_vars.values())[0].shape if len(ds.data_vars)>0 else None}")
    try:
        ds.to_netcdf(out_path)
    except Exception as e:
        print(f"  ERROR writing {out_path}: {e}")

def main():
    if len(sys.argv) < 3:
        print("Usage: python grib_to_daily_nc.py input_dir out_dir")
        sys.exit(1)

    input_dir = sys.argv[1]
    out_dir = sys.argv[2]
    safe_makedirs(out_dir)

    # find grib files
    grib_files = sorted([
        os.path.join(input_dir, f)
        for f in os.listdir(input_dir)
        if f.lower().endswith((".grib", ".grb", ".grib2", ".grb2"))
    ])

    if not grib_files:
        print("No GRIB files found in input_dir.")
        sys.exit(1)

    print(f"Found {len(grib_files)} GRIB files. Processing...")

    # store per-day temporary structure:
    # perday_store[daykey][varname] = list of (valid_dt, array, lat2d, lon2d)
    perday_store = defaultdict(lambda: defaultdict(list))

    for gf in grib_files:
        print(f"\nProcessing file: {gf}")
        if HAVE_PYGRIB:
            process_file_pygrib(gf, perday_store)
        else:
            process_file_cfgrib(gf, perday_store)

    # now assemble per day and write one netcdf per day
    for daykey in sorted(perday_store.keys()):
        print(f"\nAssembling day {daykey}")
        assemble_and_write_day(daykey, perday_store[daykey], out_dir)

    print("\nAll done.")

if __name__ == "__main__":
    main()
